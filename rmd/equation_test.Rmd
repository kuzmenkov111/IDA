
In the previous article ([Benchmark Example in MLR Part I](http://jaehyeon-kim.github.io/r/2015/01/24/Benchmark-Example-in-MLR-Part-I/)), SVM and logistic regression are benchmarked on German Credit data. For SVM, the cost parameter (C) is tuned by repeated cross-validation before the test measure is compared to that of logistic regression. 

A potential issue on that approach is that *the CV error can be optimistically biased to estimate the expected test error* as discussed in [Varma & Simon (2006)](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1397873/) and [Tibshirani and Tibshirani (2009)](http://www.stat.cmu.edu/~ryantibs/papers/cvbias.pdf). In this article, the issue is briefly summarized in its nature, remedies and cases where it can be drastic. Among the two remedies, **nested cross-validation** is performed as (1) **mlr** provides this resampling strategy and (2) it can be applied to other topics such as feasure selection.

### Summary of optimistic bias of CV error

**CV error can be optimistically biased to estimate the expected test error ([Tibshirani and Tibshirani (2009)](http://www.stat.cmu.edu/~ryantibs/papers/cvbias.pdf))**

*CV estimate of expected test error or CV error curve*

$CV(\theta)=\frac{1}{n}\sum_{k=1}^K\sum_{i\in C_{k}}L\left(y_{i},\hat{f}^{-k}\left(x_i,\theta\right)\right)$

*CV error in the kth fold or the error curve computed from the predictions in the kth fold*

$e_{k}(\theta)=\frac{1}{n_k}\sum_{i\in C_{k}}L\left(y_{i},\hat{f}^{-k}\left(x_i,\theta\right)\right)$

Therefore

- $e_{k}(\hat{\theta})\approx CV(\hat{\theta})$
    - Yes, since both are error curves evaluated at their minima.
- For fixed $\theta$, $e_{k}(\hat{\theta})\approx E\Big[ L\left(y,\hat{f}\left(x,\hat{\theta}\right)\right)\Big]$
    - **Not perfect.** 
    - RHS: $\left(x,y\right)$ is stochastically independent of the training data and hence of $\hat{\theta}$.
    - LHS: $\left(x_{i},y_{i}\right)$ has some dependence on $\hat{\theta}$ as $\hat{\theta}$ is chosen to minimize the validation error across all folds, including the kth fold.

[Tibshirani and Tibshirani (2009)](http://www.stat.cmu.edu/~ryantibs/papers/cvbias.pdf) show that the bias itself is only an issue when $p\gg N$ and its magnitude varies considerably depending on the
classifier. Therefore it can be misleading to compare the CV error rates when choosing between models.

In order th tackle down this issue, [Varma & Simon (2006)](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1397873/) suggest **nested cross-validation** to eliminate the dependence in LHS. However this strategy is computationally intensive and can be impractical. 

[Tibshirani and Tibshirani (2009)](http://www.stat.cmu.edu/~ryantibs/papers/cvbias.pdf) propose a method for the estimation of this bias that uses information from the cross-validation process. Specifically

$\hat{Bias}=\frac{1}{K}\sum_{k=1}^K[e_{k}(\hat{\theta})-e_{k}(\hat{\theta}_k)]$

$CV(\hat{\theta})=\frac{1}{K}\sum_{k=1}^Ke_{k}(\hat{\theta})$ if the fold sizes are equal

$\Rightarrow CV(\hat{\theta})+\hat{Bias}=2CV(\hat{\theta})-\frac{1}{K}\sum_{k=1}^Ke_{k}(\hat{\theta}_k)$











